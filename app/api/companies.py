import csv
import io
import re
from flask import Blueprint, request, jsonify, current_app, Response
from flask_jwt_extended import jwt_required, get_jwt
from marshmallow import Schema, fields, validates, ValidationError
from app.extensions import db
from app.models.company import Company


bp = Blueprint('companies', __name__)


class CreateCompanySchema(Schema):
    name = fields.Str(required=True, error_messages={
        "required": "Company name is required"
    })
    linkedin_url = fields.Str(required=True, error_messages={
        "required": "LinkedIn URL is required"
    })

    @validates('name')
    def validate_name(self, value):
        if len(value.strip()) < 2:
            raise ValidationError("Company name must be at least 2 characters")
        if len(value) > 255:
            raise ValidationError("Company name must be less than 255 characters")

    @validates('linkedin_url')
    def validate_linkedin_url(self, value):
        # Basic LinkedIn company URL validation
        # Accepts forms like: https://www.linkedin.com/company/<slug>/ ... or linkedin.com/company/<slug>
        import re
        pattern = r"^(https?:\/\/)?(www\.)?linkedin\.com\/company\/[A-Za-z0-9-_.%]+\/?(.*)?$"
        if not re.match(pattern, value.strip()):
            raise ValidationError("Invalid LinkedIn company URL")


create_company_schema = CreateCompanySchema()


class UpdateCompanySchema(Schema):
    name = fields.Str(required=False)
    linkedin_url = fields.Str(required=False)
    is_active = fields.Boolean(required=False)

    @validates('name')
    def validate_name(self, value):
        if value is None:
            return
        if len(value.strip()) < 2:
            raise ValidationError("Company name must be at least 2 characters")
        if len(value) > 255:
            raise ValidationError("Company name must be less than 255 characters")

    @validates('linkedin_url')
    def validate_linkedin_url(self, value):
        if value is None:
            return
        import re
        pattern = r"^(https?:\/\/)?(www\.)?linkedin\.com\/company\/[A-Za-z0-9-_.%]+\/?(.*)?$"
        if not re.match(pattern, value.strip()):
            raise ValidationError("Invalid LinkedIn company URL")


@bp.route('', methods=['POST'])
@jwt_required()
def create_company():
    """Create a LinkedIn company to track for the current tenant.

    Flow:
    1. Receive: name, linkedin_url
    2. Validate: LinkedIn URL format
    3. Check: company not already added by this tenant (by linkedin_url)
    4. Create: company record with tenant_id from JWT
    5. Return: 201 with company object
    """

    data = request.get_json() or {}

    try:
        validated = create_company_schema.load(data)
    except ValidationError as err:
        return jsonify({"error": "Validation failed", "details": err.messages}), 400

    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        # Should not happen if tokens are generated by our backend, but guard anyway
        return jsonify({"error": "Unauthorized"}), 401

    name = validated['name'].strip()
    linkedin_url = validated['linkedin_url'].strip()

    # Duplicate check per tenant (by linkedin_url)
    existing = Company.query.filter_by(tenant_id=tenant_id, linkedin_url=linkedin_url).first()
    if existing:
        return jsonify({"error": "Company already added for this tenant"}), 400

    company = Company(
        tenant_id=tenant_id,
        name=name,
        linkedin_url=linkedin_url,
        is_active=True,
    )

    db.session.add(company)
    db.session.commit()

    company_obj = {
        "company_id": company.company_id,
        "name": company.name,
        "linkedin_url": company.linkedin_url,
        "is_active": company.is_active,
        "created_at": company.created_at.isoformat() if company.created_at else None,
    }

    return jsonify({"company": company_obj}), 201


@bp.route('', methods=['GET'])
@jwt_required()
def list_companies():
    """List companies for the current tenant with pagination and filtering.

    Query Params:
      - page: int (default 1)
      - limit: int (default 20, max 100)
      - is_active: bool (optional) -> true/false
    """

    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        return jsonify({"error": "Unauthorized"}), 401

    # Pagination params
    try:
        page = int((request.args.get('page') or '1').strip())
    except Exception:
        page = 1
    if page < 1:
        page = 1

    try:
        limit = int((request.args.get('limit') or '20').strip())
    except Exception:
        limit = 20
    if limit < 1:
        limit = 20
    if limit > 100:
        limit = 100

    # Optional is_active filter
    is_active_param = request.args.get('is_active')
    query = Company.query.filter_by(tenant_id=tenant_id)
    if is_active_param is not None:
        val = is_active_param.strip().lower()
        if val in ('true', '1', 'yes'):
            query = query.filter(Company.is_active.is_(True))
        elif val in ('false', '0', 'no'):
            query = query.filter(Company.is_active.is_(False))
        # else: ignore invalid values and return unfiltered

    total = query.count()

    items = (
        query
        .order_by(Company.created_at.desc())
        .offset((page - 1) * limit)
        .limit(limit)
        .all()
    )

    companies = []
    for c in items:
        companies.append({
            "company_id": c.company_id,
            "name": c.name,
            "linkedin_url": c.linkedin_url,
            "is_active": c.is_active,
            "created_at": c.created_at.isoformat() if c.created_at else None,
        })

    # Return array and minimal pagination metadata
    return jsonify({
        "companies": companies,
        "page": page,
        "limit": limit,
        "total": total
    }), 200


@bp.route('/<company_id>', methods=['PATCH'])
@jwt_required()
def update_company(company_id):
    """Update a company's fields for the current tenant.

    Allowed fields: name, linkedin_url, is_active
    Validates tenant ownership and prevents duplicate linkedin_url per tenant.
    """

    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        return jsonify({"error": "Unauthorized"}), 401

    company = Company.query.filter_by(company_id=company_id).first()
    if not company:
        return jsonify({"error": "Company not found"}), 404

    if company.tenant_id != tenant_id:
        return jsonify({"error": "Forbidden"}), 403

    data = request.get_json() or {}
    try:
        validated = UpdateCompanySchema().load(data)
    except ValidationError as err:
        return jsonify({"error": "Validation failed", "details": err.messages}), 400

    # If linkedin_url provided, ensure not duplicate for this tenant (excluding current)
    new_linkedin = validated.get('linkedin_url')
    if new_linkedin:
        existing = Company.query.filter(
            Company.tenant_id == tenant_id,
            Company.linkedin_url == new_linkedin,
            Company.company_id != company.company_id
        ).first()
        if existing:
            return jsonify({"error": "Company already added for this tenant"}), 400

    if 'name' in validated:
        company.name = validated['name'].strip()
    if 'linkedin_url' in validated:
        company.linkedin_url = validated['linkedin_url'].strip()
    if 'is_active' in validated:
        company.is_active = bool(validated['is_active'])

    db.session.commit()

    company_obj = {
        "company_id": company.company_id,
        "name": company.name,
        "linkedin_url": company.linkedin_url,
        "is_active": company.is_active,
        "created_at": company.created_at.isoformat() if company.created_at else None,
    }

    return jsonify({"company": company_obj}), 200


@bp.route('/<company_id>', methods=['DELETE'])
@jwt_required()
def delete_company(company_id):
    """Soft delete a company (set is_active=false) for the current tenant."""

    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        return jsonify({"error": "Unauthorized"}), 401

    company = Company.query.filter_by(company_id=company_id).first()
    if not company:
        return jsonify({"error": "Company not found"}), 404

    if company.tenant_id != tenant_id:
        return jsonify({"error": "Forbidden"}), 403

    company.is_active = False
    db.session.commit()

    company_obj = {
        "company_id": company.company_id,
        "name": company.name,
        "linkedin_url": company.linkedin_url,
        "is_active": company.is_active,
        "created_at": company.created_at.isoformat() if company.created_at else None,
    }

    return jsonify({"company": company_obj}), 200


@bp.route('/<company_id>/scrape', methods=['POST'])
@jwt_required()
def scrape_company(company_id):
    """Trigger LinkedIn post scraping for a company using Apify.
    
    Query Params or Body:
      - max_posts (int, optional, default: 100): Maximum number of posts to scrape (1-1000)
    
    Returns job_id immediately (async task).
    """
    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        return jsonify({"error": "Unauthorized"}), 401
    
    # Verify company exists and belongs to tenant
    company = Company.query.filter_by(company_id=company_id).first()
    if not company:
        return jsonify({"error": "Company not found"}), 404
    
    if company.tenant_id != tenant_id:
        return jsonify({"error": "Forbidden"}), 403
    
    if not company.is_active:
        return jsonify({"error": "Company is inactive"}), 400
    
    # Get max_posts from query params or request body
    max_posts = 100  # default
    if request.args.get('max_posts'):
        try:
            max_posts = int(request.args.get('max_posts'))
            if max_posts < 1:
                max_posts = 1
            if max_posts > 1000:  # Reasonable limit
                max_posts = 1000
        except ValueError:
            return jsonify({"error": "max_posts must be a valid integer"}), 400
    
    # Also check request body (if Content-Type is application/json)
    data = {}
    if request.is_json:
        try:
            data = request.get_json() or {}
        except Exception:
            # If JSON parsing fails, just use empty dict
            data = {}
    
    if data.get('max_posts'):
        try:
            max_posts = int(data.get('max_posts'))
            if max_posts < 1:
                max_posts = 1
            if max_posts > 1000:
                max_posts = 1000
        except ValueError:
            return jsonify({"error": "max_posts must be a valid integer"}), 400
    
    # Import task here to avoid circular imports
    from app.tasks.scraper import scrape_company_posts
    from app.tasks.celery_app import celery_app
    from app.models.job import Job
    from datetime import datetime
    import uuid
    import os
    
    # Create job record
    job_id = str(uuid.uuid4())
    job = Job(
        job_id=job_id,
        tenant_id=tenant_id,
        job_type='company_scrape',
        status='pending',
        total_items=max_posts
    )
    db.session.add(job)
    db.session.commit()
    
    # Validate Celery broker is configured before attempting to queue task
    broker_url = celery_app.conf.broker_url
    if not broker_url or broker_url == 'memory://' or 'localhost' in broker_url:
        # Check if we're in production (Render sets certain env vars)
        is_production = os.getenv('FLASK_ENV') == 'production' or os.getenv('RENDER')
        if is_production:
            job.status = 'failed'
            job.error_message = f"Celery broker not properly configured. Broker URL: {broker_url}"
            job.completed_at = datetime.utcnow()
            db.session.commit()
            return jsonify({
                "error": "Failed to start scraping job",
                "details": f"Celery broker not properly configured. Broker URL: {broker_url}. "
                          "Please ensure REDIS_URL, CELERY_BROKER_URL, or CELERY_RESULT_BACKEND "
                          "is set in your environment variables."
            }), 500
    
    # Start async Celery task
    try:
        # Test broker connection before queuing task
        try:
            # Try to get broker connection to verify it's working
            broker = celery_app.broker_connection()
            if broker is None:
                raise ValueError("Celery broker connection is None")
        except Exception as conn_error:
            job.status = 'failed'
            job.error_message = f"Cannot connect to Redis broker: {str(conn_error)}"
            job.completed_at = datetime.utcnow()
            db.session.commit()
            return jsonify({
                "error": "Failed to start scraping job",
                "details": f"Cannot connect to Redis broker: {str(conn_error)}. "
                          f"Broker URL: {broker_url}. "
                          "Please ensure: 1) Redis service is running, 2) REDIS_URL is correct, "
                          "3) Worker service is created and running."
            }), 500
        
        task = scrape_company_posts.delay(job_id, tenant_id, company_id, max_posts)
        return jsonify({
            "message": "Scraping job started",
            "job_id": job_id,
            "company_id": company_id,
            "max_posts": max_posts,
            "status_url": f"/api/jobs/{job_id}"
        }), 202
    except AttributeError as e:
        # Handle 'NoneType' object has no attribute 'Redis' error
        if 'Redis' in str(e) or 'NoneType' in str(e):
            job.status = 'failed'
            job.error_message = f"Redis connection error: {str(e)}"
            job.completed_at = datetime.utcnow()
            db.session.commit()
            return jsonify({
                "error": "Failed to start scraping job",
                "details": f"Redis connection error: {str(e)}. "
                          "This usually means: 1) Celery worker service is not running, "
                          "2) Redis connection failed, or 3) Broker URL is incorrect. "
                          f"Current broker URL: {broker_url}. "
                          "Please create a Celery worker service in Render (see CREATE_WORKER_NOW.md)"
            }), 500
        raise
    except Exception as e:
        error_msg = str(e)
        # Check if it's a connection-related error
        job.status = 'failed'
        job.error_message = f"Failed to start job: {error_msg}"
        job.completed_at = datetime.utcnow()
        db.session.commit()
        if 'connection' in error_msg.lower() or 'redis' in error_msg.lower():
            return jsonify({
                "error": "Failed to start scraping job",
                "details": f"Connection error: {error_msg}. "
                          "Please ensure: 1) Redis service is running, "
                          "2) Worker service exists and is running, "
                          "3) REDIS_URL is correctly configured."
            }), 500
        return jsonify({
            "error": "Failed to start scraping job",
            "details": error_msg
        }), 500


def _parse_csv(file_storage):
    """Parse CSV file from file storage object.
    
    Returns:
        tuple: (rows, error_message) where rows is list of lists or None if error
    """
    try:
        content = file_storage.read()
        if isinstance(content, bytes):
            content = content.decode('utf-8-sig')  # Handle BOM
        stream = io.StringIO(content)
        reader = csv.reader(stream)
        rows = [row for row in reader if any(cell.strip() for cell in row)]  # Skip empty rows
        return rows, None
    except csv.Error as e:
        current_app.logger.error(f"CSV parsing error: {str(e)}")
        return None, "Invalid CSV format"
    except Exception as e:
        current_app.logger.error(f"Error reading CSV file: {str(e)}")
        return None, "Invalid CSV format"


def _normalize_linkedin_company_url(raw_url: str):
    """Validate and normalize LinkedIn company URL.
    
    Args:
        raw_url: Raw URL string from CSV
        
    Returns:
        tuple: (normalized_url, error_message) where error_message is None if valid
    """
    if not raw_url or not raw_url.strip():
        return None, "LinkedIn URL cannot be empty"
    
    url = raw_url.strip()
    
    # Add https:// if missing
    if not url.lower().startswith(('http://', 'https://')):
        url = f"https://{url}"
    
    # Validate format using the same pattern as CreateCompanySchema
    pattern = r"^(https?:\/\/)?(www\.)?linkedin\.com\/company\/[A-Za-z0-9-_.%]+\/?(.*)?$"
    if not re.match(pattern, url):
        return None, "Invalid LinkedIn company URL format. Expected: https://www.linkedin.com/company/<slug>/"
    
    # Normalize to standard format
    # Extract company slug
    match = re.search(r'/company/([A-Za-z0-9-_.%]+)', url)
    if not match:
        return None, "Invalid LinkedIn company URL format"
    
    company_slug = match.group(1)
    normalized = f"https://www.linkedin.com/company/{company_slug}/"
    
    return normalized, None


def _extract_company_data_from_row(row, header_map=None):
    """Extract company data from CSV row.
    
    Args:
        row: List of CSV cell values
        header_map: Dict mapping column names to indices (if header row exists)
        
    Returns:
        tuple: (linkedin_url, name, notes, error_message)
    """
    if header_map:
        # CSV has header row
        linkedin_idx = header_map.get('linkedin_url')
        if linkedin_idx is None or linkedin_idx >= len(row):
            return None, None, None, "linkedin_url column missing"
        
        linkedin_url = row[linkedin_idx].strip() if linkedin_idx < len(row) else ''
        name = None
        notes = None
        
        if 'name' in header_map:
            name_idx = header_map['name']
            if name_idx is not None and name_idx < len(row):
                name = row[name_idx].strip() or None
        
        if 'notes' in header_map:
            notes_idx = header_map.get('notes')
            if notes_idx is not None and notes_idx < len(row):
                notes = row[notes_idx].strip() or None
        
        return linkedin_url, name, notes, None
    else:
        # Simple CSV: first column is URL, optional second column is name
        if not row or len(row) == 0:
            return None, None, None, "Empty row"
        
        linkedin_url = row[0].strip() if len(row) > 0 else ''
        name = row[1].strip() if len(row) > 1 and row[1].strip() else None
        notes = row[2].strip() if len(row) > 2 and row[2].strip() else None
        
        return linkedin_url, name, notes, None


@bp.route('/bulk-upload', methods=['POST'])
@jwt_required()
def bulk_upload_companies():
    """Bulk upload multiple LinkedIn company URLs via CSV file.
    
    Accepts CSV file with:
    - Simple format: One URL per line (first column)
    - Header format: Columns: linkedin_url, name, notes
    
    Supports up to 1000 companies per upload.
    Skips duplicates (doesn't fail entire upload).
    Returns summary with added/skipped/failed counts.
    """
    current_app.logger.debug("Bulk upload companies: Request received")
    
    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        current_app.logger.warning("Bulk upload companies: Missing tenant_id in JWT token")
        return jsonify({"error": "Unauthorized"}), 401
    
    current_app.logger.debug(f"Bulk upload companies: Processing for tenant_id={tenant_id}")
    
    # Check for file
    uploaded_file = request.files.get('file')
    if not uploaded_file:
        current_app.logger.warning("Bulk upload companies: No file provided")
        return jsonify({"error": "No file provided"}), 400
    
    # Validate file type
    if not uploaded_file.filename or not uploaded_file.filename.lower().endswith('.csv'):
        current_app.logger.warning(f"Bulk upload companies: Invalid file type: {uploaded_file.filename}")
        return jsonify({"error": "File must be CSV format"}), 400
    
    current_app.logger.debug(f"Bulk upload companies: Parsing CSV file: {uploaded_file.filename}")
    
    # Parse CSV
    rows, parse_error = _parse_csv(uploaded_file)
    if parse_error:
        current_app.logger.error(f"Bulk upload companies: CSV parse error: {parse_error}")
        return jsonify({"error": parse_error}), 400
    
    if not rows:
        current_app.logger.warning("Bulk upload companies: Empty CSV file")
        return jsonify({"error": "Invalid CSV format"}), 400
    
    current_app.logger.debug(f"Bulk upload companies: Parsed {len(rows)} rows")
    
    # Detect header row
    header_map = None
    first_row_lower = [cell.strip().lower() for cell in rows[0]]
    if 'linkedin_url' in first_row_lower:
        header_map = {name: idx for idx, name in enumerate(first_row_lower)}
        data_rows = rows[1:]
        current_app.logger.debug("Bulk upload companies: Detected header row with columns: %s", list(header_map.keys()))
    else:
        data_rows = rows
        current_app.logger.debug("Bulk upload companies: No header row detected, using simple format")
    
    if not data_rows:
        current_app.logger.warning("Bulk upload companies: No data rows found")
        return jsonify({"error": "No company rows found in CSV"}), 400
    
    # Check row limit (count data rows, not including header)
    if len(data_rows) > 1000:
        current_app.logger.warning(f"Bulk upload companies: Too many rows: {len(data_rows)}")
        return jsonify({"error": "Maximum 1000 companies allowed"}), 400
    
    current_app.logger.debug(f"Bulk upload companies: Processing {len(data_rows)} data rows")
    
    # Get existing companies for duplicate check
    existing_companies = Company.query.filter_by(tenant_id=tenant_id).all()
    existing_urls = {c.linkedin_url for c in existing_companies}
    current_app.logger.debug(f"Bulk upload companies: Found {len(existing_urls)} existing companies for tenant")
    
    # Process rows
    normalized_in_batch = set()  # Track URLs in current batch to avoid duplicates within batch
    to_insert = []
    errors = []
    skipped = 0
    
    for idx, row in enumerate(data_rows, start=1):
        linkedin_url, name, notes, row_error = _extract_company_data_from_row(row, header_map)
        
        if row_error:
            current_app.logger.debug(f"Bulk upload companies: Row {idx} extraction error: {row_error}")
            errors.append({"row": idx, "error": row_error})
            continue
        
        # Normalize and validate URL
        normalized_url, normalize_error = _normalize_linkedin_company_url(linkedin_url)
        if normalize_error:
            current_app.logger.debug(f"Bulk upload companies: Row {idx} URL validation error: {normalize_error}")
            errors.append({"row": idx, "error": normalize_error})
            continue
        
        # Check for duplicates
        if normalized_url in existing_urls or normalized_url in normalized_in_batch:
            current_app.logger.debug(f"Bulk upload companies: Row {idx} skipped (duplicate): {normalized_url}")
            skipped += 1
            continue
        
        # Generate company name if not provided
        if not name or not name.strip():
            # Extract company slug from URL as fallback name
            match = re.search(r'/company/([A-Za-z0-9-_.%]+)', normalized_url)
            if match:
                name = match.group(1).replace('-', ' ').replace('_', ' ').title()
            else:
                name = "Unknown Company"
        
        # Validate name length
        name = name.strip()
        if len(name) < 2:
            name = "Unknown Company"
        if len(name) > 255:
            name = name[:255]
        
        # Create company object
        company = Company(
            tenant_id=tenant_id,
            name=name,
            linkedin_url=normalized_url,
            is_active=True
        )
        to_insert.append(company)
        normalized_in_batch.add(normalized_url)
    
    current_app.logger.debug(
        f"Bulk upload companies: Processed rows - to_insert={len(to_insert)}, "
        f"skipped={skipped}, errors={len(errors)}"
    )
    
    # Check if we have anything to process
    if not to_insert and not errors and skipped == 0:
        current_app.logger.warning("Bulk upload companies: No valid companies found")
        return jsonify({"error": "No valid companies found in CSV"}), 400
    
    # Batch insert with transaction
    added = 0
    try:
        if to_insert:
            current_app.logger.debug(f"Bulk upload companies: Inserting {len(to_insert)} companies")
            db.session.add_all(to_insert)
            db.session.commit()
            added = len(to_insert)
            current_app.logger.info(
                f"Bulk upload companies: Successfully added {added} companies for tenant_id={tenant_id}"
            )
    except Exception as exc:
        db.session.rollback()
        current_app.logger.exception(f"Bulk upload companies: Database error: {str(exc)}")
        return jsonify({"error": "Database error occurred"}), 500
    
    # Prepare summary
    summary = {
        "added": added,
        "skipped": skipped,
        "failed": len(errors),
        "errors": errors,
        "total_rows": len(data_rows)
    }
    
    current_app.logger.info(
        f"Bulk upload companies: Summary - added={added}, skipped={skipped}, "
        f"failed={len(errors)}, total_rows={len(data_rows)}"
    )
    
    return jsonify(summary), 201


@bp.route('/bulk-upload/template', methods=['GET'])
@jwt_required()
def download_bulk_upload_template():
    """Download CSV template for bulk company uploads.
    
    Template includes example row with linkedin_url, name, and notes columns.
    """
    current_app.logger.debug("Bulk upload template: Request received")
    
    claims = get_jwt()
    tenant_id = claims.get('tenant_id')
    if not tenant_id:
        current_app.logger.warning("Bulk upload template: Missing tenant_id in JWT token")
        return jsonify({"error": "Unauthorized"}), 401
    
    current_app.logger.debug(f"Bulk upload template: Generating template for tenant_id={tenant_id}")
    
    # Create CSV template
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header row
    writer.writerow(['linkedin_url', 'name', 'notes'])
    
    # Write example row
    writer.writerow([
        'https://www.linkedin.com/company/openai/',
        'OpenAI',
        'AI research company'
    ])
    
    # Create response
    response = Response(output.getvalue(), mimetype='text/csv')
    response.headers['Content-Disposition'] = 'attachment; filename=company_upload_template.csv'
    
    current_app.logger.debug("Bulk upload template: Template generated successfully")
    
    return response

